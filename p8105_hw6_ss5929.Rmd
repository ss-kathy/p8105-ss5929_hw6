---
title: "hw6_ss5929"
author: "Shuyi SHEN"
date: "11/15/2019"
output: github_document
---

```{r}
library(tidyverse)
library(broom)
library(modelr)
library(ggplot2)


```

# Problem 1
```{r}
birthweight = read.csv("birthweight.csv") %>%
        janitor::clean_names() %>%
  mutate(babysex = factor(babysex),
         malform = factor(malform),
         frace   = factor(frace),
         mrace   = factor(mrace))
  

is.na(birthweight) %>% summary()
```
*comment*

**birthweight**

The dataset includes `r ncol(birthweight)` variables and `r nrow(birthweight)` observations. There are four variables including babysex, malform, frace and mrace we need to factor and there are no NA values after checking.


```{r,message = FALSE,warning=FALSE}
fit = lm(bwt~.,data = birthweight)
summary(fit)

fit_0 = lm(bwt~babysex+bhead+blength+delwt+gaweeks+parity+smoken,data = birthweight)
```
*comment*

After doing t-test to all variables, I build the model by choosing those having significant effect on bwt according to p-value. For those dummy variables, only one level is significant will be omitted.

```{r,message = FALSE,warning=FALSE}
birthweight %>% 
add_residuals(fit_0) %>% 
add_predictions(fit_0) %>% 
  ggplot(aes(x=pred,y=resid))+geom_point()+
  geom_smooth(method = "lm")+
  labs(
    x = "predictions",
    y = "residuals",
    title = "The distribution of predictions and residuals"
  )

```
*comment*

According the plot, we can see the points are approximately around 0, when predictions are larger, residuals will decrease.


## compare the model
```{r,warning=FALSE}
fit_1 = lm(bwt~blength+gaweeks,data=birthweight)
fit_2 = lm(bwt~bhead+blength+babysex, data=birthweight)

cv_df=crossv_mc(birthweight,100)

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble
cv_df %>% pull(test) %>% .[[1]] %>% as_tibble

cv_df=
  cv_df %>% 
  mutate(
    train = map(train,as_tibble),
    test = map(test,as_tibble)
  )


cv_df = 
  cv_df %>% 
  mutate(mod_0  = map(train,
                      ~lm(bwt~babysex+bhead+blength+delwt+fincome+                                            frace+gaweeks+mheight+momage+mrace+parity+ppbmi+smoken,data=.x)),
         mod_1  = map(train, ~lm(bwt~blength+gaweeks,data=.x)),
         mod_2  = map(train, ~lm(bwt~bhead+blength+babysex, data=.x))) %>% 
  mutate(rmse_0 = map2_dbl(mod_0, test, ~rmse(model = .x, data = .y)),
         rmse_1 = map2_dbl(mod_1, test, ~rmse(model = .x, data = .y)),
         rmse_2 = map2_dbl(mod_2, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

```

*comment*

According to the plot of rmse, we can see that the model_0 will have least rmse which will be prefered. Model_1 has the highest rmse which will increase bias. However, we can also see that modeL_0 has a little bit greater variance which can be due to much predictors.

# Problem 2
read data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

```


```{r,message = FALSE, warning=FALSE}

boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}


weather_df= weather_df %>% select(tmax,tmin)

boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )


results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample,~lm(tmax~tmin,data = .x)),
    a      = map(models,broom::glance),
    b      = map(models,broom::tidy),
  ) %>% 
  select(-models,-strap_sample) %>%
  unnest_wider(a) %>% 
  select(r.squared,b) %>% 
  unnest_wider(b) %>% 
  select(r.squared,estimate) %>% 
  unnest_wider(estimate)

result_plot=results %>% 
            rename(r2 = r.squared) %>% 
            rename(b1 = ...1) %>% 
            rename(b2 = ...2) %>% 
            mutate(l = log(b1*b2)) 

quantile(result_plot$r2,probs = c(0.025,0.975))
quantile(result_plot$l,probs = c(0.025,0.975))
```

*comment*

The confidence interval for r^2 is (0.89,0.93) and log(beta0*beta1) is (1.96,2.05).

```{r,message = FALSE}

result_plot %>% 
  ggplot(aes(x=r2))+geom_density()+
  labs(
    title = "The distribution of r^2"
  )

result_plot %>% 
  ggplot(aes(x=l))+geom_density()+
  labs(
    title = "The distribution of log(beta0*beta1)"
  )

```

*comment*

From the plot, we can see the distribution of r^2 and log(beta0*beta1) are nearly normal distribution with left skew.

